{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"tweets.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 refers positive statements, 1 is negative statemens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing id column\n",
    "data.drop(['id'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for class balance\n",
    "data['label'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting label counts\n",
    "data['label'].value_counts().plot(kind='pie',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'].value_counts().plot(kind='bar',)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA CLEANING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#substituting 's with \"is\"\n",
    "re.sub(r\"'s\\b\", \"is\", data['tweet'][24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove user mentions\n",
    "data['tweet'][11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keeping the aphabets and numbers\n",
    "re.sub(r'[^a-zA-Z0-9]', ' ', data['tweet'][11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the hashtags\n",
    "data['tweet'][0]\n",
    "re.sub(\"#\", \"\", data['tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the links\n",
    "re.sub(r\"http\\S+\", \"\", data['tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing punctuations\n",
    "data['tweet'][25]\n",
    "re.sub(r\"[^a-zA-Z]\", \"\", data['tweet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removal of STOP WORDS\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))\n",
    "print(nltk_stopwords)\n",
    "len(nltk_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "sklearn_stopwords = set(ENGLISH_STOP_WORDS)\n",
    "print(sklearn_stopwords)\n",
    "len(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#common stopwords from NLTK & skLearn\n",
    "print(nltk_stopwords.intersection(sklearn_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nltk_stopwords.intersection(sklearn_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine stopwords from skLearn & NLTK\n",
    "combined_stopwords = nltk_stopwords.union(sklearn_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEXT NORMALIZATION:Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tweet'][63].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = ''\n",
    "for token in data['tweet'][63].split():\n",
    "    new_sent = new_sent + lemmatizer.lemmatize(token.lower()) + ' '\n",
    "\n",
    "new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install contractions\n",
    "import contractions\n",
    "data['tweet'][24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions.fix(data['tweet'][24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DEFINING THE CLEANER FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaner_without_stopwords(text):\n",
    "    new_text = re.sub(r\"'s\\b\", \" is\", text)\n",
    "    new_text = re.sub(\"#\", \"\", new_text)\n",
    "    new_text = re.sub(\"@[A-Za-z0-9]+\", \"\", new_text)\n",
    "    new_text = re.sub(r\"http\\S+\", \"\", new_text)\n",
    "    new_text = contractions.fix(new_text)    \n",
    "    new_text = re.sub(r\"[^a-zA-Z]\", \" \", new_text)    \n",
    "    new_text = new_text.lower().strip()\n",
    "    \n",
    "    cleaned_text = ''\n",
    "    for token in new_text.split():\n",
    "        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = []  # list of cleaned tweets\n",
    "for twt in data['tweet']:\n",
    "    cleaned_tweets.append(tweet_cleaner_without_stopwords(twt))\n",
    "    \n",
    "print(cleaned_tweets[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets[24]\n",
    "data['tweet'][1500]\n",
    "cleaned_tweets[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweets_w/o_SW'] = cleaned_tweets\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting 25 most common words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting all words from the dataset\n",
    "all_words = []\n",
    "for t in data['tweet']:\n",
    "    all_words.extend(t.split())\n",
    "\n",
    "print(all_words[:50])\n",
    "len(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Top 25 most common words')\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "freq_dist.plot(25, cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PLOT FOR CLEANED TWEETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for t in data['cleaned_tweets_w/o_SW']:\n",
    "    all_words.extend(t.split())\n",
    "\n",
    "print(all_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(all_words)) # this is the number of unique words in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency Distribution\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Top 25 most common words')\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "freq_dist.plot(25, cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DV After applying Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(combined_stopwords)\n",
    "def tweet_cleaner_with_stopwords(text):\n",
    "    new_text = re.sub(r\"'s\\b\", \" is\", text)\n",
    "    new_text = re.sub(\"#\", \"\", new_text)\n",
    "    new_text = re.sub(\"@[A-Za-z0-9]+\", \"\", new_text)\n",
    "    new_text = re.sub(r\"http\\S+\", \"\", new_text)\n",
    "    new_text = contractions.fix(new_text)    \n",
    "    new_text = re.sub(r\"[^a-zA-Z]\", \" \", new_text)    \n",
    "    new_text = new_text.lower().strip()\n",
    "    \n",
    "    new_text = [token for token in new_text.split() if token not in combined_stopwords]\n",
    "    \n",
    "    new_text = [token for token in new_text if len(token)>2]\n",
    "    \n",
    "    cleaned_text = ''\n",
    "    for token in new_text:\n",
    "        cleaned_text = cleaned_text + lemmatizer.lemmatize(token) + ' '\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = list(data['tweet'].apply(tweet_cleaner_with_stopwords))\n",
    "print(cleaned_tweets[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_tweets_with_SW'] = cleaned_tweets\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for t in data['cleaned_tweets_with_SW']:\n",
    "    all_words.extend(t.split())\n",
    "\n",
    "print(all_words[:50])\n",
    "\n",
    "# Frequency Distribution\n",
    "freq_dist = nltk.FreqDist(all_words)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title('Top 25 most common words')\n",
    "plt.xticks(fontsize=15)\n",
    "\n",
    "freq_dist.plot(25, cumulative=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_stopwords = ['phone', 'mobile', 'twitter', 'rt', 'com', 'follow']\n",
    "final_stopwords = domain_stopwords + list(combined_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BOW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CV = CountVectorizer()\n",
    "CV_features = CV.fit_transform(data['cleaned_tweets_w/o_SW'])\n",
    "CV_features.shape\n",
    "CV_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(CV_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_features[0].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(CV_features.todense() )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.size/1e6\n",
    "import numpy as np\n",
    "np.count_nonzero(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*np.count_nonzero(df)/df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_features[10].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.inverse_transform(np.asarray(CV_features[10].todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(CV_features, data['label'], test_size=0.25, stratify=data['label'], random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression(solver='liblinear')\n",
    "LR.fit(X_train, y_train)\n",
    "\n",
    "print(LR.score(X_train, y_train))  # train score)\n",
    "print(LR.score(X_test, y_test))   # test score)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CROSS VALIDATING THE EXISTING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = CV_features\n",
    "y = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_val_score(LR1, X, y, cv=kfold, scoring='accuracy')\n",
    "print(results)\n",
    "print(np.round((results.mean()) * 100, 2), np.round((results.std()) * 100, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THE ACCURACY IS 88.23 +/- 0.93 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "results = cross_validate(LR1, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean()) * 100, 2), np.round((results['train_score'].std()) * 100, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean()) * 100, 2), np.round((results['test_score'].std()) * 100, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#HYPER PARAMETER TRAINING FOR OPTIMAL RESULTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1')\n",
    "\n",
    "C_values = np.arange(0.00001, 1, 0.05) # 20 values\n",
    "\n",
    "grid = GridSearchCV(estimator=LR1, param_grid={'C': C_values}, cv=kfold, scoring='accuracy', \\\n",
    "                    return_train_score=True, verbose=2, n_jobs=-1)\n",
    "grid_results = grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.best_params_, grid_results.best_score_, grid_results.best_index_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_test_score'][grid_results.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_train_score'][grid_results.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['std_test_score'][grid_results.best_index_]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_train_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_results.cv_results_['mean_train_score'] - grid_results.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.param_grid['C'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results.cv_results_['mean_train_score'] - grid_results.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a pipeline & cross-validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "X = data['cleaned_tweets_w/o_SW']\n",
    "y = data['label']\n",
    "\n",
    "CV = CountVectorizer()\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)\n",
    "\n",
    "CV_pipe = Pipeline([('CV', CV) , ('LR', LR1)] )\n",
    "\n",
    "results = cross_val_score(CV_pipe, X, y, cv=kfold, scoring='accuracy')\n",
    "print(np.round((results.mean())*100, 2), np.round((results.std())*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CV_pipe['CV'].vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#USE OF N-GRAM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigrams + bigrams\n",
    "X = data['cleaned_tweets_w/o_SW']\n",
    "y = data['label']\n",
    "\n",
    "# we want to include only those words in the vocab which have min df of 5,\n",
    "# means select only those words which occur ATLEAST in 5 documents!! \n",
    "# AND SELECT the TOP 1000 FEATURES ONLY to build the model\n",
    "CV = CountVectorizer(stop_words=final_stopwords,  ngram_range=(1, 2), min_df=5)\n",
    "\n",
    "LR1 = LogisticRegression(class_weight='balanced', solver='liblinear', penalty='l1', C=0.4)\n",
    "CV_pipe = Pipeline([('CV', CV) , ('LR', LR1)] )\n",
    "results = cross_validate(CV_pipe, X, y, cv=kfold, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# print(results['train_score'])\n",
    "print(np.round((results['train_score'].mean())*100, 2), np.round((results['train_score'].std())*100, 2)) \n",
    "\n",
    "# print(results['test_score'])\n",
    "print(np.round((results['test_score'].mean())*100, 2), np.round((results['test_score'].std())*100, 2)) \n",
    "\n",
    "CV.fit_transform(X)\n",
    "len(CV.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(CV.get_feature_names_out())\n",
    "LR1.fit(CV.fit_transform(X), y)\n",
    "coef = LR1.coef_\n",
    "mglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coef.ravel()), len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 2\n",
    "\n",
    "LR1.fit(CV.fit_transform(X), y)\n",
    "coef = LR1.coef_\n",
    "\n",
    "# visualize only 2-gram features\n",
    "mglearn.tools.visualize_coefficients(coef.ravel()[mask], feature_names[mask], n_top_features=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.to_pickle(\"tweets_cleaned.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#WORD EMBEDDING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fingerprint',\n",
       " 'pregnancy',\n",
       " 'test',\n",
       " 'android',\n",
       " 'apps',\n",
       " 'beautiful',\n",
       " 'cute',\n",
       " 'health',\n",
       " 'igers',\n",
       " 'iphoneonly',\n",
       " 'iphonesia',\n",
       " 'iphone']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_list = list(data['cleaned_tweets_w/o_SW'].apply(lambda x: x.split()))\n",
    "tweets_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "cbow_model = Word2Vec(tweets_list, vector_size = 300, window = 3, min_count=5, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=2420, vector_size=300, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "print(cbow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iphone',\n",
       " 'apple',\n",
       " 'i',\n",
       " 'my',\n",
       " 'the',\n",
       " 'to',\n",
       " 'a',\n",
       " 'is',\n",
       " 'samsung',\n",
       " 'it',\n",
       " 'and',\n",
       " 'you',\n",
       " 'new',\n",
       " 'twitter',\n",
       " 'for',\n",
       " 'com',\n",
       " 'phone',\n",
       " 'me',\n",
       " 'sony',\n",
       " 'not']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_model.wv.index_to_key[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2420"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbow_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    \n",
    "    # doc1 contains those words of the document which are included in the vocab\n",
    "    doc1 = [word for word in doc.split() if word in cbow_model.wv.index_to_key]\n",
    "    \n",
    "    wv1 = []  # this will contain the WE of all the vocab words from the doc\n",
    "    for word in doc1:\n",
    "        wv1.append(cbow_model.wv.get_vector(word))\n",
    "    wv1_ = np.array(wv1)\n",
    "    wv1_mean = wv1_.mean(axis=0)\n",
    "    return wv1_mean\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
